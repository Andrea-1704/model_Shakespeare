{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6oluApoAazeDNnwsrRqdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea-1704/model_Shakespeare/blob/main/Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "noXM5AEXtMuK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "seq_size = 32\n",
        "embedding_size = 64\n",
        "lstm_size = 64\n",
        "gradients_norm = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "21K2Pma8tYDH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the dataset:"
      ],
      "metadata": {
        "id": "hFFEXCffabCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"kingburrito666/shakespeare-plays\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hheJsUs_YrBc",
        "outputId": "df45d688-33e3-42d3-d46a-89a665caf699"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(path)\n",
        "file_path = os.path.join(path, \"Shakespeare_data.csv\")"
      ],
      "metadata": {
        "id": "N39HuTweZEXX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataset inside doc:"
      ],
      "metadata": {
        "id": "BeXqayTSagnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (file_path, 'r') as f:\n",
        "    doc = f.read()"
      ],
      "metadata": {
        "id": "psp4ZJ8OaD1g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "TX9z3tDHvIYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data split"
      ],
      "metadata": {
        "id": "JFAOoTPV825L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from a CSV file (update the path as necessary)\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to create unique keys for play and character\n",
        "def play_and_character(play, character):\n",
        "    return f\"{play}_{character}\"\n",
        "\n",
        "# Function to split the dataset into train, validation, and test sets\n",
        "def split_dataset_by_character(data, test_fraction=0.2, val_fraction=0.1):\n",
        "    \"\"\"\n",
        "    Splits the dataset into train, validation, and test sets, keeping character data separate.\n",
        "\n",
        "    data: DataFrame with columns [Play, Player, PlayerLine]\n",
        "    test_fraction: Percentage of data for the test set\n",
        "    val_fraction: Percentage of data for the validation set (relative to the total)\n",
        "\n",
        "    Returns: tuple (train_data, val_data, test_data)\n",
        "    \"\"\"\n",
        "    skipped_characters = 0\n",
        "    all_train = []\n",
        "    all_val = []\n",
        "    all_test = []\n",
        "\n",
        "    grouped = data.groupby([\"Play\", \"Player\"])\n",
        "\n",
        "    for (play, character), group in grouped:\n",
        "        if len(group) <= 2:\n",
        "            skipped_characters += 1\n",
        "            continue\n",
        "\n",
        "        examples = group\n",
        "        train, test = train_test_split(examples, test_size=test_fraction, random_state=42)\n",
        "        if val_fraction > 0:\n",
        "            train, val = train_test_split(train, test_size=val_fraction / (1 - test_fraction), random_state=42)\n",
        "            all_val.append(val)\n",
        "        all_train.append(train)\n",
        "        all_test.append(test)\n",
        "\n",
        "    train_data = pd.concat(all_train, ignore_index=True)\n",
        "    val_data = pd.concat(all_val, ignore_index=True) if all_val else pd.DataFrame()\n",
        "    test_data = pd.concat(all_test, ignore_index=True)\n",
        "\n",
        "    print(f\"Skipped characters: {skipped_characters}\")\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "train_data, val_data, test_data = split_dataset_by_character(data, test_fraction=0.2, val_fraction=0.1)\n",
        "\n",
        "# Save the resulting datasets\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "val_data.to_csv(\"val_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0OhFBw0840C",
        "outputId": "f705ca62-1f07-40ef-d08b-95cb7632d169"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped characters: 161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(path)"
      ],
      "metadata": {
        "id": "I7K0BrUMB_06"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"train_data.csv\", 'r') as f:\n",
        "    doc_train = f.read()\n",
        "with open (\"test_data.csv\", 'r') as f:\n",
        "    doc_test = f.read()\n",
        "with open (\"val_data.csv\", 'r') as f:\n",
        "    doc_val = f.read()"
      ],
      "metadata": {
        "id": "IzAKV1yuCC9F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert the all dataset into a list of words. We remove undesired characters."
      ],
      "metadata": {
        "id": "mbZ54q3K0_GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_words(doc):\n",
        "    lines = doc.split('\\n')\n",
        "    lines = [line.strip(r'\\\"') for line in lines]\n",
        "    words = ' '.join(lines).split()\n",
        "    return words"
      ],
      "metadata": {
        "id": "CXoit6KJvG7W"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that is defined to remove all the punctuation characters."
      ],
      "metadata": {
        "id": "3qRW66LX1MwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(words):\n",
        "    punct = set(string.punctuation)\n",
        "    words = [''.join([char for char in list(word) if char not in punct]) for word in words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "_-cDL7KzvLKt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to build a vocabulary of each word considering the number of occurrencies of that word."
      ],
      "metadata": {
        "id": "8EfrASwY1a2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get vocab from word list\n",
        "def getvocab(words):\n",
        "    wordfreq = Counter(words)\n",
        "    sorted_wordfreq = sorted(wordfreq, key=wordfreq.get)\n",
        "    return sorted_wordfreq"
      ],
      "metadata": {
        "id": "AMGsTJagvM9L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the training we create 2 dictionaies that map each word to a distinct number and viceversa."
      ],
      "metadata": {
        "id": "6uFbl3pX1zrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_map(vocab):\n",
        "    int_to_vocab = {k:w for k,w in enumerate(vocab)}\n",
        "    vocab_to_int = {w:k for k,w in int_to_vocab.items()}\n",
        "    return int_to_vocab, vocab_to_int"
      ],
      "metadata": {
        "id": "GIAzmh2QvPF0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function that concatenates all these functions together:"
      ],
      "metadata": {
        "id": "p6IIskm6-kZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_custom(text):\n",
        "    words = extract_words(text)\n",
        "    words = remove_punct(words)\n",
        "    return words"
      ],
      "metadata": {
        "id": "5WKludjm-kHK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use these functions for our dataset:"
      ],
      "metadata": {
        "id": "nULwGANg1-6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_train = remove_punct(extract_words(doc_train)) # words without punctuation\n",
        "words_test = remove_punct(extract_words(doc_test))\n",
        "words_val = remove_punct(extract_words(doc_val))\n",
        "vocab_train = getvocab(words_train)\n",
        "vocab_test = getvocab(words_test)\n",
        "vocab_val = getvocab(words_val)\n",
        "int_to_vocab_train, vocab_to_int_train = vocab_map(vocab_train)\n",
        "int_to_vocab_test, vocab_to_int_test = vocab_map(vocab_test)\n",
        "int_to_vocab_val, vocab_to_int_val = vocab_map(vocab_val)"
      ],
      "metadata": {
        "id": "VgkDJyy9vRB-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create batches of sequences:"
      ],
      "metadata": {
        "id": "Hsadiiuy2heM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(words, vocab_to_int, batch_size, seq_size):\n",
        "    # generate a Xs and Ys of shape (batchsize * num_batches) * seq_size\n",
        "    word_ints = [vocab_to_int[word] for word in words]\n",
        "    num_batches = int(len(word_ints) / (batch_size * seq_size))\n",
        "    Xs = word_ints[:num_batches*batch_size*seq_size]\n",
        "    Ys = np.zeros_like(Xs)\n",
        "    Ys[:-1] = Xs[1:]\n",
        "    Ys[-1] = Xs[0]\n",
        "    Xs = np.reshape(Xs, (num_batches*batch_size, seq_size))\n",
        "    Ys= np.reshape(Ys, (num_batches*batch_size, seq_size))\n",
        "\n",
        "    # iterate over rows of Xs and Ys to generate batches\n",
        "    for i in range(0, num_batches*batch_size, batch_size):\n",
        "        yield Xs[i:i+batch_size, :], Ys[i:i+batch_size, :]"
      ],
      "metadata": {
        "id": "LPKcVAgavTgG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6RrVs44DvYYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, n_vocab=90, seq_length=80, embedding_dim=8, lstm_hidden_size=256):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "\n",
        "        # Embedding layer: maps each character to an 8-dimensional space\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_dim)\n",
        "        #the first parameter is the total number of words inside the vocabulary\n",
        "        #the second is the length of the vector for each instance\n",
        "        #in practice each batch get converted into a dense representation of an\n",
        "        #embedding\n",
        "\n",
        "        # Two LSTM layers, each with 256 nodes\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Dense softmax output layer to predict next character probabilities\n",
        "        self.dense = nn.Linear(lstm_hidden_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        # Embed input characters into learned 8-dimensional space\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pass through LSTM layers\n",
        "        lstm_out, state = self.lstm(embedded, prev_state)\n",
        "\n",
        "        # Dense layer to produce logits for the output vocabulary\n",
        "        logits = self.dense(lstm_out)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "        # Initialize LSTM hidden and cell states with zeros\n",
        "        return (torch.zeros(2, batch_size, self.lstm_hidden_size),  # Hidden states for 2 layers\n",
        "                torch.zeros(2, batch_size, self.lstm_hidden_size))  # Cell states for 2 layers\n"
      ],
      "metadata": {
        "id": "X5Sd5Ip1cMWy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last layer of the ShakespeareRNN is a fully connected layer (self.dense = nn.Linear(lstm_hidden_size, n_vocab)) that outputs the logits for each character in the vocabulary.\n",
        "\n",
        "The softmax function is not explicitly applied in the model. Instead, the logits from the FC layer are directly passed to the loss function. This is because CrossEntropyLoss internally applies log_softmax during computation."
      ],
      "metadata": {
        "id": "aH_ozXAAtT10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer"
      ],
      "metadata": {
        "id": "l3EMhY3_vbn1"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words))"
      ],
      "metadata": {
        "id": "0UMuFiCbveHT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ZzaUtEP3vjwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints utils functions\n",
        "During training is important to save the progress in case google colab interrupts the execution. To do so we decided to define a function that save the checkpoint and, in particular, save the copy of the values of the current parameters inside the network, the loss value, the epoch reached and the parameters of the optimizer."
      ],
      "metadata": {
        "id": "RdSXi5y7m8rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We mantain a default path to store these values:"
      ],
      "metadata": {
        "id": "OqINBwqVnwL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"shakespeare_rnn_checkpoint.pth\""
      ],
      "metadata": {
        "id": "iXYDxEX6n4z4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, path=checkpoint_path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)"
      ],
      "metadata": {
        "id": "66PB7nS1nnbL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, path=checkpoint_path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "        print(f\"Checkpoint loaded: epoch {epoch}, loss {loss:.4f}.\")\n",
        "        return epoch, loss\n",
        "    else:\n",
        "        print(\"No checkpoint found.\")\n",
        "        return 0, None"
      ],
      "metadata": {
        "id": "tlZBBoxOn_HQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function for training"
      ],
      "metadata": {
        "id": "MRErZt9gsW4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(words, vocab_to_int, int_to_vocab, n_vocab, n_epochs = 5):\n",
        "    print(\"training started\")\n",
        "    # RNN instance\n",
        "    net = ShakespeareRNN(n_vocab, seq_size, embedding_size, lstm_size)\n",
        "    net = net.to(device)\n",
        "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        batches = get_batches(words, vocab_to_int, batch_size, seq_size)\n",
        "        print(\"batches taken\")\n",
        "        state_h, state_c = net.init_state(batch_size)\n",
        "\n",
        "        # Transfer data to GPU\n",
        "        state_h = state_h.to(device)\n",
        "        state_c = state_c.to(device)\n",
        "        for x, y in batches:\n",
        "            iteration += 1\n",
        "\n",
        "            # Tell it we are in training mode\n",
        "            net.train()\n",
        "\n",
        "            # Reset all gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Transfer data to GPU\n",
        "            x = torch.tensor(x).to(device)\n",
        "            y = torch.tensor(y).to(device)\n",
        "\n",
        "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss_value = loss.item()\n",
        "\n",
        "            # Perform back-propagation\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
        "\n",
        "            # Update the network's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            #save the current values of training:\n",
        "            if iteration % 100 == 0:\n",
        "                save_checkpoint(net, optimizer, e, loss)\n",
        "                print('Epoch: {}/{}'.format(e, n_epochs),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n",
        "\n",
        "            # if iteration % 1000 == 0:\n",
        "                # predict(device, net, flags.initial_words, n_vocab,vocab_to_int, int_to_vocab, top_k=5)\n",
        "                # torch.save(net.state_dict(),'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "6Vwtip-kviwO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparam tuning\n",
        "Here's an implementation to perform hyperparameter tuning for learning rate, weight decay, and learning rate scheduler hyperparameters. It uses grid search to identify the best combination of parameters."
      ],
      "metadata": {
        "id": "bBbkVsNot7hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the parameters grid:"
      ],
      "metadata": {
        "id": "cVeJFP3JuTnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'learning_rate': [0.05, 0.01, 0.005, 0.001],\n",
        "    'weight_decay': [1e-5, 5e-5, 1e-4],\n",
        "    'lr_scheduler': [\n",
        "        (\"StepLR (step_size=num_epochs//3, gamma=0.1)\",\n",
        "         lambda optimizer, num_epochs: torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 3, gamma=0.1)),\n",
        "\n",
        "        (\"CosineAnnealingLR (T_max=num_epochs//3, eta_min=1e-4)\",\n",
        "         lambda optimizer, num_epochs: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs // 3, eta_min=1e-4)),\n",
        "\n",
        "        (\"ExponentialLR (gamma=0.95)\",\n",
        "         lambda optimizer, _: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)),\n",
        "\n",
        "        (\"ExponentialLR (gamma=0.9)\",\n",
        "         lambda optimizer, _: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9))\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "kvVI6sYruQwU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_search(param_grid, train_fn, n_vocab, seq_size, embedding_size, lstm_size, words, vocab_to_int, int_to_vocab, num_epochs):\n",
        "    \"\"\"Perform hyperparameter tuning using grid search.\"\"\"\n",
        "    best_params = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Create all combinations of hyperparameters\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    for params in param_combinations:\n",
        "        # Initialize the model, optimizer, and scheduler with the current parameters\n",
        "        net = ShakespeareRNN(n_vocab, seq_size, embedding_size, lstm_size).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
        "\n",
        "        # Initialize the scheduler using the factory function\n",
        "        scheduler_name, scheduler_factory = params['lr_scheduler']\n",
        "        scheduler = scheduler_factory(optimizer, num_epochs)\n",
        "\n",
        "        # Train the model and obtain the final loss\n",
        "        print(f\"Testing parameters: {params} with scheduler: {scheduler_name}\")\n",
        "        trained_model, final_loss = train_fn(\n",
        "            words, vocab_to_int, int_to_vocab, n_vocab, num_epochs\n",
        "        )\n",
        "\n",
        "        # Update the best parameters if a better loss is found\n",
        "        if final_loss < best_loss:\n",
        "            best_loss = final_loss\n",
        "            best_params = deepcopy(params)\n",
        "\n",
        "    print(f\"Best parameters: {best_params} with loss: {best_loss}\")\n",
        "    return best_params"
      ],
      "metadata": {
        "id": "1zlPdg-UuWYK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = hyperparameter_search(\n",
        "    param_grid,\n",
        "    train_rnn,  #pass the name of the function that compute the training\n",
        "    len(vocab_val),  #vocabulary size\n",
        "    seq_size=80,  #sequence size\n",
        "    embedding_size=8,  #embedding size\n",
        "    lstm_size=256,  #LSTM size\n",
        "    words=words_val,  #training data\n",
        "    vocab_to_int=vocab_to_int_val,  #vocabulary to int mapping\n",
        "    int_to_vocab=int_to_vocab_val,  #int to vocabulary mapping\n",
        "    num_epochs = 20\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "WLci7MqpumRc",
        "outputId": "40a92e8f-2e08-4703-a4d4-fcb330bb2ab3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters: {'learning_rate': 0.05, 'weight_decay': 1e-05, 'lr_scheduler': ('StepLR (step_size=num_epochs//3, gamma=0.1)', <function <lambda> at 0x7dd29818e830>)} with scheduler: StepLR (step_size=num_epochs//3, gamma=0.1)\n",
            "training started\n",
            "batches taken\n",
            "batches taken\n",
            "Epoch: 1/20 Iteration: 100 Loss: 8.421089172363281\n",
            "batches taken\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-eba8042f069e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m best_params = hyperparameter_search(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#pass the name of the function that compute the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseq_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#sequence size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-671d06e23fd5>\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(param_grid, train_fn, n_vocab, seq_size, embedding_size, lstm_size, words, vocab_to_int, int_to_vocab, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Train the model and obtain the final loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing parameters: {params} with scheduler: {scheduler_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         trained_model, final_loss = train_fn(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-49-99b115ae8fb2>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(words, vocab_to_int, int_to_vocab, n_vocab, n_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Transfer data to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_net = train_rnn(words_train, vocab_to_int_train, int_to_vocab_train, len(vocab_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "ssIgJlwTvmVl",
        "outputId": "262d99b7-1978-4e11-c6d3-5ed00dfa39f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started\n",
            "batches taken\n",
            "Epoch: 0/5 Iteration: 100 Loss: 10.151239395141602\n",
            "Epoch: 0/5 Iteration: 200 Loss: 9.759001731872559\n",
            "Epoch: 0/5 Iteration: 300 Loss: 9.128534317016602\n",
            "Epoch: 0/5 Iteration: 400 Loss: 9.18450927734375\n",
            "Epoch: 0/5 Iteration: 500 Loss: 8.906452178955078\n",
            "batches taken\n",
            "Epoch: 1/5 Iteration: 600 Loss: 7.715320110321045\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b49b2e40600a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-6d70849a863d>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(words, vocab_to_int, int_to_vocab, n_vocab)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Perform back-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(device, rnn_net, ['hey', 'you'], len(vocab_test), vocab_to_int_test, int_to_vocab_test)"
      ],
      "metadata": {
        "id": "0JrbQs8WUAC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}